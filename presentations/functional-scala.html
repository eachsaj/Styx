<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Fast Data</title>

    <meta name="description" content="Fast Data">
    <meta name="author" content="Bas Geerdink">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/bas.css" id="theme">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);

    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->

</head>

<!-- Projector—1280×720 native resolution, HDMI connections, Screen—16:9 aspect ratio-->

<body>
<div class="reveal">

    <!-- Any section element inside of this container is displayed as a slide -->
    <div class="slides">
        <section>
            <h2>Streaming Analytics</h2>
            <h3>with Scala and Spark Structured Streaming</h3>
            <img src="img/functional_scala.png" style="height: 250px;"/>
            <p>
                <small>Bas Geerdink | December 13, 2019 | Functional Scala </small>
            </p>
            <aside class="notes">
                Streaming analytics (or fast data) is becoming an increasingly popular subject in enterprise
                organizations because customers want to have real-time experiences, such as notifications and advice
                based on their online behavior and other users’ actions. A typical streaming analytics solution follows
                a “pipes and filters” pattern that consists of three main steps: detecting patterns on raw event data
                (complex event processing), evaluating the outcomes with the aid of business rules and machine learning
                algorithms, and deciding on the next action.

                Bas Geerdink details an open source reference solution for streaming analytics that covers many use
                cases that follow this pattern: actionable insights, fraud detection, log parsing, traffic analysis,
                factory data, the IoT, and others. The solution is built with the KFC stack: Kafka, Flink, and
                Cassandra. All source code is written in Scala.

                Bas explores a few architecture challenges that arise when dealing with streaming data, such as latency
                issues, event time versus server time, and exactly once processing. He provides architectural diagrams,
                explanations, a demo, and the source code. The solution (“Styx”) is open source and available on GitHub.
            </aside>
        </section>

        <section>
            <h2>Who am I?</h2>
            <pre><code data-trim>
                { 
                  "name": "Bas Geerdink",
                  "role": "Technology Lead",
                  "background": ["Artificial Intelligence",
                                 "Informatics"],
                  "mixins": ["Software engineering",
                             "Architecture",
                             "Management",
                             "Innovation"],
                  "twitter": "@bgeerdink",
                  "linked_in": "bgeerdink"
                }
            </code></pre>
        </section>

        <section>
            <h2>Big Data</h2>
            <ul>
                <li>Volume</li>
                <li>Variety</li>
                <li>Velocity</li>
            </ul>
        </section>

        <section>
            <h2>Fast Data Use Cases</h2>
            <small>
                <table>
                    <thead>
                    <tr>
                        <th>Sector</th>
                        <th>Data source</th>
                        <th>Pattern</th>
                        <th>Notification</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>Finance</td>
                        <td>Payment data</td>
                        <td>Fraud detection</td>
                        <td>Block money transfer</td>
                    </tr>
                    <tr>
                        <td>Finance</td>
                        <td>Clicks and page visits</td>
                        <td>Trend analysis</td>
                        <td>Actionable insights</td>
                    </tr>
                    <tr>
                        <td>Insurance</td>
                        <td>Page visits</td>
                        <td>Customer is stuck in a web form</td>
                        <td>Chat window</td>
                    </tr>
                    <tr>
                        <td>Healthcare</td>
                        <td>Patient data</td>
                        <td>Heart failure</td>
                        <td>Alert doctor</td>
                    </tr>
                    <tr>
                        <td>Traffic</td>
                        <td>Cars passing</td>
                        <td>Traffic jam</td>
                        <td>Update route info</td>
                    </tr>
                    <tr>
                        <td>Internet of Things</td>
                        <td>Machine logs</td>
                        <td>System failure</td>
                        <td>Alert to sys admin</td>
                    </tr>
                    </tbody>
                </table>
            </small>
            <aside class="notes">
                Mention: IoT, increase of data, importance of analytics, relation with big data.<br> Definition:
                streaming
                is a type of data processing engine that is designed with infinite data sets in mind (Tyler Akidau).<br>
                Real-time
                doesn't really exist; so, rather call it <i>fast data</i> processing.
            </aside>
        </section>

        <section>
            <h2>Apache Spark libraries</h2>
            <img src="img/spark.png" class="stretch">
        </section>

        <section>
            <h2>Streaming Architecture</h2>
            <img src="img/architecture_spark.png" class="stretch">
        </section>

        <section>
            <h2>Spark-Kafka integration</h2>
            <ul>
                <li>A Fast Data application is a running <i>job</i> that processes events in a data store (Kafka)</li>
                <li>Jobs can be deployed as ever-running pieces of software in a big data cluster (Spark)</li>
                <li class="fragment" data-fragment-index="1">The basic pattern of a job is:
                    <ul class="fragment" data-fragment-index="1">
                        <li>Connect to the stream and consume events</li>
                        <li>Group and gather events (windowing)</li>
                        <li>Perform analysis (aggregation) on each window</li>
                        <li>Write the result to another stream (sink)</li>
                    </ul>
                </li>
            </ul>
        </section>

        <section>
            <h2>Parallelism</h2>
            <ul>
                <li>To get high throughput, we have to process the events in parallel</li>
                <li>Parallelism can be configured on cluster level (YARN) and on job level (number of worker threads)
                </li>
                <!--                        <li>http://spark.apache.org/docs/latest/configuration.html</li>-->
            </ul>

            <!--                    <pre><code data-trim>-->
            <!--                    spark.executor.cores=2, spark.task.cpus=1-->
            <!--                    </code></pre>-->

            <pre><code data-trim>
  val conf = new SparkConf()
    .setMaster("local[8]")
    .setAppName("FraudNumberOfTransactions")
                    </code></pre>

            <pre><code data-trim>
./bin/spark-submit --name "LowMoneyAlert" --master local[4]
  --conf "spark.dynamicAllocation.enabled=true"
  --conf "spark.dynamicAllocation.maxExecutors=2" styx.jar
                    </code></pre>
        </section>

        <section>
            <h2>Hello speed!</h2>
            <!-- Simplified code: serializing data and making a DataStream of 'raw' Kafka events -->
            <pre><code data-trim>
  // connect to Spark
  val spark = SparkSession
    .builder
    .config(conf)
    .getOrCreate()

  // for using DataFrames
  import spark.sqlContext.implicits._

  // get the data from Kafka: subscribe to topic
  val df = spark
    .readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "localhost:9092")
    .option("subscribe", "transactions")
    .option("startingOffsets", "latest")
    .load()
                </code></pre>
        </section>

        <section>
            <h2>Event time</h2>
            <ul>
                <li>Events occur at certain time <span class="fragment fade-in" data-fragment-index="1">
                                <b>&rAarr; event time</b></span></li>
                <!--                        <li>... but arrive a little while later <span class="fragment fade-in" data-fragment-index="1">-->
                <!--                                <b>&rAarr; ingestion time</b></span></li>-->
                <li>... and are processed later <span class="fragment fade-in" data-fragment-index="1">
                                <b>&rAarr; processing time</b></span></li>
            </ul>
            <img src="img/EventTime.png" class="fragment" data-fragment-index="2"/>

            <aside class="notes">
                Events can reach us <i>out-of-order</i>; by looking at the event time we can still detect the original
                order</li>
            </aside>
        </section>

        <section>
            <h2>Out-of-orderness</h2>
            <img src="img/EventTime_StarWars.png" class="stretch"/><br>
            <img src="img/Star_Wars_logo.png" style="width:200px;"/>
        </section>

        <section>
            <h2>Windows</h2>
            <ul>
                <li>In processing infinite streams, we usually look at a time <i>window</i></li>
                <li>A windows can be considered as a <i>bucket of time</i></li>
                <li class="fragment" data-fragment-index="1">There are different types of windows:
                    <ul class="fragment" data-fragment-index="1">
                        <li>Sliding window</li>
                        <!-- e.g. looking back at the last hour, continuous updating average of system activity -->
                        <li>Tumbling window</li> <!-- e.g. processing data per day to get a financial report -->
                        <li><i>Session window</i></li>
                        <!-- based on event data, e.g. users logged in and out. Not in Spark! -->
                    </ul>
                </li>
                <aside class="notes">
                    Windows can be very large (e.g. weeks) and thereby gather a lot of state. At the end of a window, a
                    function (Window operator)
                    is called.
                    In Spark: a window evaluation is an _action_ (as opposed to transformation).
                </aside>
            </ul>
        </section>

        <section>
            <h2>Windows</h2>
            <img src="img/Windows.png" class="stretch"/>
        </section>

        <section>
            <h2>Window considerations</h2>
            <ul>
                <li><b>Size</b>: large windows lead to big state and long calculations</li>
                <li><b>Number</b>: many windows (e.g. sliding, session) lead to more calculations</li>
                <li><b>Evaluation</b>: do all calculations within one window, or keep a cache across multiple windows
                    (e.g. when comparing windows, like in trend analysis)
                </li>
                <li><b>Timing</b>: events for a window can appear <i>early</i> or <i>late</i></li>
            </ul>
            <aside class="notes">
                <p>Fraud detection: sliding window of 1 day</p>
                <p>Actionable insights: tumbling window of 1 hour</p>
            </aside>
        </section>

        <section>
            <h2>Windows</h2>
            <p>Example: sliding window of 1 day, evaluated every 15 minutes over the field 'customer_id'.
                The event time is stored in the field 'transaction_time'</p>
            <pre><code data-trim>
  // aggregate, produces a sql.DataFrame
  val windowedTransactions = transactionStream
    .groupBy(
      window($"transaction_time", "1 day", "15 minutes"),
      $"customer_id")
    .agg(count("t_id") as "count", $"customer_id", $"window.end")
                    </code></pre>

            <aside class="notes">
                !! groupBy, keyBy = divide across nodes?
            </aside>
        </section>

        <section>
            <h2>Watermarks</h2>
            <ul>
                <li><i>Watermarks</i> are timestamps that trigger the computation of the window</li>
                <li>They are generated at a time that allows a bit of slack for <i>late events</i></li>
                <li class="fragment" data-fragment-index="1">Any event that reaches the processor later
                    than the watermark, but with an event time that should belong to the former window, is
                    <i>ignored</i></li>
                <!--                        <li class="fragment" data-fragment-index="2">In Flink, it's possible to allow late events to trigger re-computation-->
                <!--                            of a window by setting the <i>allowedLateness</i> property</li>-->
            </ul>
        </section>

        <section>
            <h2>Event Time and Watermarks</h2>
            <p>Example: sliding window of 60 seconds, evaluated every 30 seconds.
                The watermark is set at 1 second, giving all events some time to arrive.</p>
            <pre><code data-trim>
  val windowedTransactions = transactionStream
    .withWatermark("created_at", "1 second")
    .groupBy(
      window($"transaction_time", "60 seconds", "30 seconds"),
      $"customer_id")
    .agg(...)  // e.g. count/sum/...
                    </code></pre>
            <aside class="notes">
                Kafka is a replayable data source. The checkpoint is made in a write-ahead log.
            </aside>
        </section>

        <section>
            <h2>Fault-tolerance and checkpointing</h2>
            <p></p>
            <ul>
                <li>Data is in one of three stages:</li>
                <ul>
                    <li><b>Unprocessed</b> <span class="fragment fade-in" data-fragment-index="1">
                            &rAarr; Kafka consumers provide <i>offsets</i> that guarantee no data loss for unprocessed data</span>
                    </li>
                    <li><b>In transit</b> <span class="fragment fade-in" data-fragment-index="1">
                            &rAarr; data can be preserved in a <i>checkpoint</i>, to reload and replay it after a crash</span>
                    </li>
                    <li><b>Processed</b> <span class="fragment fade-in" data-fragment-index="1">
                            &rAarr; Kafka provides an acknowledgement once data is written</span></li>
                </ul>
            </ul>
        </section>

        <section>
            <h2>Sink the output to Kafka</h2>
            <pre><code data-trim>
 businessEvents
    .format("kafka")
    .option("kafka.bootstrap.servers", "localhost:9092")
    .option("topic", "business_events")
    .option("checkpointLocation", "/hdfs/checkpoint")
    .start()  // this triggers the start of the streaming query
                    </code></pre>
        </section>

        <section>
            <h2>Model scoring</h2>
            <ul>
                <li>To determine the follow-up action of a aggregated business event (e.g. pattern), we have to enrich
                    the event with customer data
                </li>
                <li>The resulting data object contains the characteristics (features) as input for a model</li>
                <li class="fragment" data-fragment-index="1">To get the features and score the model, efficiency plays a
                    role again:
                </li>
                <ul class="fragment" data-fragment-index="1">
                    <li>Direct database call > API call</li>
                    <li>In-memory model cache > model on disk</li>
                </ul>
            </ul>
        </section>

        <section>
            <h2>PMML</h2>
            <ul>
                <li>PMML is the glue between data science and data engineering</li>
                <li>Data scientists can export their machine learning models to PMML (or PFA) format</li>
            </ul>
            <pre><code data-trim>
from sklearn.linear_model import LogisticRegression
from sklearn2pmml import sklearn2pmml

events_df = pandas.read_csv("events.csv")

pipeline = PMMLPipeline(...)
pipeline.fit(events_df, events_df["notifications"])

sklearn2pmml(pipeline, "LogisticRegression.pmml", with_repr = True)
                </code></pre>
        </section>

        <section>
            <h2>PMML</h2>
            <img src="img/PMML.png" class="stretch"/>
        </section>

        <section>
            <h2>Model scoring</h2>
            <ul>
                <li>The models can be loaded into memory to enable split-second performance</li>
                <li>By applying <i>map</i> functions over the events we can process/transform the
                    data in the windows:
                </li>
                <ol>
                    <li>enrich each business event by getting more data</li>
                    <li>filtering events based on selection criteria (rules)</li>
                    <li>score a machine learning model on each event</li>
                    <li>write the outcome to a new event / output stream</li>
                </ol>
                </li>
            </ul>
        </section>

        <section>
            <h2>Openscoring.io</h2>
            <pre><code data-trim>
                def score(event: RichBusinessEvent, pmmlModel: PmmlModel): Double = {
                    val arguments = new util.LinkedHashMap[FieldName, FieldValue]

                    for (inputField: InputField <- pmmlModel.getInputFields.asScala) {
                        arguments.put(inputField.getField.getName,
                            inputField.prepare(customer.all(fieldName.getValue)))
                    }

                    // return the notification with a relevancy score
                    val results = pmmlModel.evaluate(arguments)

                    pmmlModel.getTargetFields.asScala.headOption match {
                        case Some(targetField) =>
                            val targetFieldValue = results.get(targetField.getName)
                        case _ => throw new Exception("No valid target")
                        }
                    }
                }
                </code></pre>
        </section>

        <section>
            <h2>Thanks!</h2>
            <p>Read more about streaming analytics at:</p>
            <ul>
                <li><a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">The world beyond batch:
                    Streaming 101</a></li>
                <li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43864.pdf">Google
                    Dataflow paper</a></li>
            </ul>
            <p>Source code and presentation are available at:</p>
            <p>
                <a href="https://github.com/streaming-analytics/Styx">https://github.com/streaming-analytics/Styx</a>
            </p>
        </section>

    </div>

</div> <!-- reveal -->

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
        // More info https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,

            transition: 'slide', // none/fade/slide/convex/concave/zoom

            // More info https://github.com/hakimel/reveal.js#dependencies
            dependencies: [
                { src: 'lib/js/classList.js', condition: function () { return !document.body.classList; } },
                { src: 'plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
                { src: 'plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
                { src: 'plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } },
                { src: 'plugin/search/search.js', async: true },
                { src: 'plugin/zoom-js/zoom.js', async: true },
                { src: 'plugin/notes/notes.js', async: true }
            ]
        });


</script>

</body>

</html>